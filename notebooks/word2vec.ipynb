{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxence Lasbordes | MASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'sentiment'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"review_ids\"] = tokenizer(\n",
    "        x[\"review\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    x[\"label\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000  # the number of training example\n",
    "\n",
    "# We first shuffle the data !\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Select 5000 samples\n",
    "dataset = dataset.select(range(n_samples))\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset = dataset.map(lambda x: preprocessing_fn(x, tokenizer))\n",
    "\n",
    "# Remove useless columns\n",
    "dataset = dataset.remove_columns([\"review\", \"sentiment\"])\n",
    "\n",
    "# Split the train and validation\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "document_train_set = split[\"train\"]\n",
    "document_valid_set = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 0, 0], [1, 3, 4, 0], [1, 2, 4, 5], [2, 3, 5, 0], [3, 4, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def extract_words_contexts(text_ids, R):\n",
    "    words = []\n",
    "    contexts = []\n",
    "    for i in range(len(text_ids)):\n",
    "        words.append(text_ids[i])\n",
    "        context = []\n",
    "        for j in range(max(0, i - R), min(len(text_ids), i + R + 1)):\n",
    "            if i != j:\n",
    "                context.append(text_ids[j])\n",
    "        if (\n",
    "            len(context) < 2 * R\n",
    "        ):  # Adding a padding token of id 0 when the context is less than 2R\n",
    "            context += [0] * (2 * R - len(context))\n",
    "        contexts.append(context)\n",
    "    return words, contexts\n",
    "\n",
    "\n",
    "words, contexts = extract_words_contexts([1, 2, 3, 4, 5], 2)\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset_to_lists(dataset, R):\n",
    "    words = []\n",
    "    contexts = []\n",
    "    for example in dataset:\n",
    "        w, c = extract_words_contexts(example[\"review_ids\"], R)\n",
    "        words.extend(w)\n",
    "        contexts.extend(c)\n",
    "    return contexts, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words: [2054, 2057, 2031, 2182, 2003, 1037, 2143, 3819, 2005, 3087]\n",
      "List of contexts: [[2057, 2031, 2182, 2003, 1037, 2143, 3819, 2005, 3087, 2008, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2054, 2031, 2182, 2003, 1037, 2143, 3819, 2005, 3087, 2008, 17257, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2054, 2057, 2182, 2003, 1037, 2143, 3819, 2005, 3087, 2008, 17257, 1999, 0, 0, 0, 0, 0, 0, 0, 0], [2054, 2057, 2031, 2003, 1037, 2143, 3819, 2005, 3087, 2008, 17257, 1999, 1996, 0, 0, 0, 0, 0, 0, 0], [2054, 2057, 2031, 2182, 1037, 2143, 3819, 2005, 3087, 2008, 17257, 1999, 1996, 2088, 0, 0, 0, 0, 0, 0], [2054, 2057, 2031, 2182, 2003, 2143, 3819, 2005, 3087, 2008, 17257, 1999, 1996, 2088, 1997, 0, 0, 0, 0, 0], [2054, 2057, 2031, 2182, 2003, 1037, 3819, 2005, 3087, 2008, 17257, 1999, 1996, 2088, 1997, 2695, 0, 0, 0, 0], [2054, 2057, 2031, 2182, 2003, 1037, 2143, 2005, 3087, 2008, 17257, 1999, 1996, 2088, 1997, 2695, 1011, 0, 0, 0], [2054, 2057, 2031, 2182, 2003, 1037, 2143, 3819, 3087, 2008, 17257, 1999, 1996, 2088, 1997, 2695, 1011, 3919, 0, 0], [2054, 2057, 2031, 2182, 2003, 1037, 2143, 3819, 2005, 2008, 17257, 1999, 1996, 2088, 1997, 2695, 1011, 3919, 2964, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Define the context window size R\n",
    "R = 10\n",
    "\n",
    "train_contexts, train_words = flatten_dataset_to_lists(document_train_set, R)\n",
    "val_contexts, val_words = flatten_dataset_to_lists(document_valid_set, R)\n",
    "\n",
    "print(f\"List of words: {train_words[:10]}\\nList of contexts: {train_contexts[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, words, contexts):\n",
    "        self.words = words\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.contexts[idx]), torch.tensor(self.words[idx]))\n",
    "\n",
    "\n",
    "train_set = DocumentDataset(train_words, train_contexts)\n",
    "val_set = DocumentDataset(val_words, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, K, vocab):\n",
    "    positive_context_ids, word_ids = zip(*batch)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    word_ids = torch.tensor(word_ids)\n",
    "    positive_context_ids = torch.stack(\n",
    "        [torch.tensor(ids) for ids in positive_context_ids]\n",
    "    )\n",
    "\n",
    "    # Generate negative samples\n",
    "    batch_size, context_size = positive_context_ids.shape\n",
    "    vocab_size = len(vocab)\n",
    "    negative_context_ids = torch.randint(\n",
    "        0, vocab_size, (batch_size, K * context_size), dtype=torch.long\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"word_ids\": word_ids,\n",
    "        \"positive_context_ids\": positive_context_ids,\n",
    "        \"negative_context_ids\": negative_context_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Parameters\n",
    "K = 5\n",
    "batch_size = 256\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda batch: collate_fn(batch, K=K, vocab=tokenizer.vocab),\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda batch: collate_fn(batch, K=K, vocab=tokenizer.vocab),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=10, K=5, batch_size=256\n",
      "- Batch 0\n",
      "    words_ids: torch.Size([256]), \n",
      "    positive_context_ids: torch.Size([256, 20]), \n",
      "    negative_context_ids: torch.Size([256, 100])\n",
      "- Batch 1\n",
      "    words_ids: torch.Size([256]), \n",
      "    positive_context_ids: torch.Size([256, 20]), \n",
      "    negative_context_ids: torch.Size([256, 100])\n",
      "- Batch 2\n",
      "    words_ids: torch.Size([256]), \n",
      "    positive_context_ids: torch.Size([256, 20]), \n",
      "    negative_context_ids: torch.Size([256, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlasb\\AppData\\Local\\Temp\\ipykernel_3140\\3112037006.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in positive_context_ids]\n"
     ]
    }
   ],
   "source": [
    "# Display a few batches\n",
    "iterations = 0\n",
    "print(f\"R={R}, K={K}, batch_size={batch_size}\")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    if iterations > 2:\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"\"\"- Batch {iterations}\n",
    "    words_ids: {batch['word_ids'].shape}, \n",
    "    positive_context_ids: {batch['positive_context_ids'].shape}, \n",
    "    negative_context_ids: {batch['negative_context_ids'].shape}\"\"\"\n",
    "    )\n",
    "    \n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # We create two embedding layers and we set the padding_idx to 0\n",
    "        self.word_embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.context_embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, word_ids, context_ids):\n",
    "        w = self.word_embed(word_ids)\n",
    "        C = self.context_embed(context_ids)\n",
    "        dot_product = torch.bmm(w.unsqueeze(1), C.transpose(1, 2)).squeeze(1)\n",
    "        return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We redo some of the previous steps to properly parametrized our training function,\n",
    "such as the dataloaders\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def training(\n",
    "    model,\n",
    "    document_train_set,\n",
    "    document_valid_set,\n",
    "    learning_rate,\n",
    "    device,\n",
    "    tokenizer,\n",
    "    B,\n",
    "    E,\n",
    "    K,\n",
    "    R,\n",
    "):\n",
    "\n",
    "    # Model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Loading the data\n",
    "    print(\"Loading the data...\")\n",
    "    train_contexts, train_words = flatten_dataset_to_lists(document_train_set, R)\n",
    "    val_contexts, val_words = flatten_dataset_to_lists(document_valid_set, R)\n",
    "    train_set = DocumentDataset(train_words, train_contexts)\n",
    "    val_set = DocumentDataset(val_words, val_contexts)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=B,\n",
    "        collate_fn=lambda batch: collate_fn(batch, K=K, vocab=tokenizer.vocab),\n",
    "    )\n",
    "    val_data_loader = DataLoader(\n",
    "        dataset=val_set,\n",
    "        batch_size=B,\n",
    "        collate_fn=lambda batch: collate_fn(batch, K=K, vocab=tokenizer.vocab),\n",
    "    )\n",
    "\n",
    "    print(f\"Number of Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Training the model\n",
    "    model.train()\n",
    "    for epoch in range(E):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            word_ids = batch[\"word_ids\"].to(device)\n",
    "            positive_context_ids = batch[\"positive_context_ids\"].to(device)\n",
    "            negative_context_ids = batch[\"negative_context_ids\"].to(device)\n",
    "\n",
    "            # Outputs of the model\n",
    "            logits_pos = model(word_ids, positive_context_ids)\n",
    "            logits_neg = model(word_ids, negative_context_ids)\n",
    "\n",
    "            # Loss\n",
    "            loss_pos = F.logsigmoid(logits_pos).mean()\n",
    "            loss_neg = F.logsigmoid(-logits_neg).mean()\n",
    "            loss = -loss_pos - loss_neg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Testing the model on the validation set\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data_loader:\n",
    "                word_ids = batch[\"word_ids\"].to(device)\n",
    "                positive_context_ids = batch[\"positive_context_ids\"].to(device)\n",
    "                negative_context_ids = batch[\"negative_context_ids\"].to(device)\n",
    "\n",
    "                # Outputs of the model\n",
    "                logits_pos = model(word_ids, positive_context_ids)\n",
    "                logits_neg = model(word_ids, negative_context_ids)\n",
    "\n",
    "                # Accuracy, we don't count the padding tokens\n",
    "                correct_predictions += (\n",
    "                    ((F.sigmoid(logits_pos) > 0.5) & (positive_context_ids != 0))\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                )\n",
    "                correct_predictions += (\n",
    "                    ((F.sigmoid(logits_neg) < 0.5) & (negative_context_ids != 0))\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                )\n",
    "                total_pos = (positive_context_ids != 0).sum().item()\n",
    "                total_neg = (negative_context_ids != 0).sum().item()\n",
    "                total_predictions += total_pos + total_neg\n",
    "\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} | Training Loss: {total_loss/len(train_dataloader)}, Test Accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    torch.save({\n",
    "        'word_embed': model.word_embed.weight.data.clone(),  # Save word embeddings\n",
    "        'context_embed': model.context_embed.weight.data.clone()  # Save context embeddings\n",
    "    }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Number of Parameters: 6104400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3631 [00:00<?, ?it/s]C:\\Users\\mlasb\\AppData\\Local\\Temp\\ipykernel_3140\\3112037006.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in positive_context_ids]\n",
      "Training: 100%|██████████| 3631/3631 [02:17<00:00, 26.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Training Loss: 3.915961238379913, Test Accuracy: 0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3631/3631 [02:15<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Training Loss: 1.4671830851753958, Test Accuracy: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3631/3631 [02:15<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Training Loss: 1.0018951136262841, Test Accuracy: 0.8739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3631/3631 [02:14<00:00, 27.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Training Loss: 0.8120698052409988, Test Accuracy: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3631/3631 [02:14<00:00, 27.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Training Loss: 0.7084791955869738, Test Accuracy: 0.8877\n",
      "Training completed!\n",
      "Model saved as model_dim-100_radius-10_ratio-2-batch-256-epoch-5.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 100\n",
    "B = 256  # Batch size\n",
    "E = 5  # Number of epochs\n",
    "K = 2  # Factor for negative samples\n",
    "R = 10  # Context window size\n",
    "\n",
    "# Model\n",
    "model = Word2Vec(vocab_size=tokenizer.vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# We can now train the model and modify the hyperparameters as we want\n",
    "\n",
    "\n",
    "training(\n",
    "    model,\n",
    "    document_train_set,\n",
    "    document_valid_set,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    B=B,\n",
    "    E=E,\n",
    "    K=K,\n",
    "    R=R,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "file_name = f\"model_dim-{embedding_dim}_radius-{R}_ratio-{K}-batch-{B}-epoch-{E}.ckpt\"\n",
    "save_model(model, file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
